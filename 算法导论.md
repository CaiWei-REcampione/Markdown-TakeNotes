[toc]

# 算法

## 输入与输出

待计算问题的任一实例,都需要以某种方式交给对应的算法,对所求解问题特定实例的这种描述统称为输入(input)

## 基本操作、确定性与可行性

所谓确定性和可行性是指,算法应可描述为由若干语义明确的基本操作组成的指令序列,且每一基本操作在对应的计算模型中均可兑现

## 有穷性与正确性

不难理解,任意算法都应在执行有限次基本操作之后终止并给出输出,此即所谓算法的有穷性(finiteness) .进一步地,算法不仅应该迟早会终止,而且所给的输出还应该能够符合由问题本身在事先确定的条件,此即所谓算法的正确性(correctness)

> 证明算法有穷性和正确性的一个重要技巧,就是从适当的角度审视整个计算过程,并找出其所具有的某种不变性和单调性.其中的单调性通常是指问题的有效规模会随着算法的推进不断递减.不变性则不仅应在算法初始状态下自然满足,而且应与最终的正确性相呼应-当问题的有效规模缩减到0时,不变性应随即等价于正确性

## 退化与鲁棒性

同一问题往往不限于一种算法,而同一算法也常常会有多种实现方式,因此除了以上必须具备的基本属性,在应用环境中还需从实用的角度对不同算法及其不同版本做更为细致考量和取舍.这些细致的要求尽管应纳入软件工程的范畴,但也不失为成熟算法的重要标志.比如其中之一就是,除一般性情况外,实用的算法还应能够处理各种极端的输入实例.算法所谓的鲁棒性(robustness) ,就是要求能够尽可能充分地应对此类情况

## 重用性

从实用角度评判不同算法及其不同实现方式时,可采用的另一标准是:算法的总体框架能否便捷地推广至其它场合.算法模式可推广并适用于不同类型基本元素的这种特性,即是重用性的一种典型形式

# 算法效率

## 可计算性

程序设计语言的目的,在于学会如何编写合法(即合乎特定程序语言的语法)的程序,从而保证编写的程序或者能够经过编译和链接生成执行代码,或者能够由解释器解释执行.然而从通过计算有效解决实际问题的角度来看,这只是第一个层次,仅仅做到语法正确还远远不够.很遗憾,算法所应具备的更多基本性质,合法的程序并非总是自然具备

## 难解性

实际上我们不仅需要确定,算法对任何输入都能够在有穷次操作之后终止,而且更加关注该过程所需的时间.很遗憾,很多算法即便满足有穷性,但在终止之前所花费的时间成本却太高

## 计算效率

首先需要确立一种尺度,用以从==时间和空间==等方面度量算法的计算成本,进而依此尺度对不同算法进行比较和评判.当然,更重要的是研究和归纳算法设计与实现过程中的一般性规律与技巧,以编写出效率更高、能够处理更大规模数据的程序

## 数据结构

无论是算法的初始输入、中间结果还是最终输出,在计算机中都可以数据的形式表示.对于数据的存储、组织、转移及变换等操作,不同计算模型和平台环境所支持的具体形式不尽相同,其执行效率将直接影响和决定算法的整体效率.数据结构这一学科正是以“数据”这"一信息的表现形式为研究对象,旨在建立支持高效算法的数据信息处理策略、技巧与方法.要做到根据实际应用需求自如地设计、实现和选用适当的数据结构,必须首先对算法设计的技巧以及相应数据结构的特性了然于心

## 复杂度度量

### 时间复杂度

即使是同一算法,对于不同的输入所需的运行时间并不相同.为针对运行时间建立起一种可行、可信的评估标准,我们不得不首先考虑其中最为关键的因素.其中,问题实例的规模往往是决定计算成本的主要因素.一般地,问题规模越接近,相应的计算成本也越接近;而随着问题规模的扩大,计算成本通常也呈上升趋势

### 渐进复杂度

对于同一问题的两个算法A和B,通过比较其时间复杂度$T_A(n)$和$T_B(n)$,即可评价二者对于同一输入规模n的计算效率高低.然而,藉此还不足以就其性能优劣做出总体性的评判,比如对于某些问题,一些算法更适用于小规模输入,而另一些则相反.幸运的是,在评价算法运行效率时,我们往往可以忽略其处理小规模问题时的能力差异,转而关注其在处理更大规模问题时的表现.其中的原因不难理解,小规模问题所需的处理时间本来就相对更少,故此时不同算法的实际效率差异并不明显;而在处理更大规模的问题时,效率的些许差异都将对实际执行效果产生巨大的影响.这种着眼长远、更为注重时间复杂度的总体变化趋势和增长速度的策略与方法,即所谓的渐进分析(asymptotic analysis) 

#### 大$\sigma$记号

同样地出于保守的估计,我们首先关注T(n)的渐进上界.为此可引入所谓“大$\sigma$记号”(big-0 notation) .具体地,若存在正的常数c和函数f(n),使得对任何n >> 2都有
$$
T(n) \leq c.f(n)
$$
则可认为在n足够大之后, f(n)给出了T(n)增长速度的一个渐进上界.此时,记之为:
$$
T(n) =\sigma(f(n))
$$
由这一定义,可导出大0记号的以下性质:

1. 对于任一常数c > 0,有$\sigma(f(n)) = \sigma(c\cdot f(n))$
2. 对于任意常数a > b > 0,有$\sigma(n^a+n^b) = \sigma(n^a)$

前一性质意味着,在大$\sigma$记号的意义下,函数各项正的常系数可以忽略并等同于1.后一性质则意味着,多项式中的低次项均可忽略,只需保留最高次项.可以看出,大$\sigma$记号的这些性质的确体现了对函数总体渐进增长趋势的关注和刻画

#### 环境差异

在实际环境中直接测得的执行时间T(n),虽不失为衡量算法性能的一种指标,但作为评判不同算法性能优劣的标准,其可信度值得推敲.事实上,即便是同一算法、同一输入,在不同的硬件平台上、不同的操作系统中甚至不同的时间,所需要的计算时间都不尽相同.因此,有必要按照超脱于具体硬件平台和软件环境的某一客观标准,来度量算法的时间复杂度,并进而评价不同算法的效率差异

#### 基本操作

一种自然且可行的解决办法是,将时间复杂度理解为算法中各条指令的执行时间之和.在图灵机(Turing Machine, TM)和随机存储机(Random Access Machine, RAM)等计算模型中,指令语句均可分解为若干次基本操作,比如算术运算、比较、分支、子程序调用与返回等;而在大多数实际的计算环境中,每一次这类基本操作都可在常数时间内完成.如此,不妨将T(n)定义为算法所执行基本操作的总次数.也就是说, T(n)决定于组成算法的所有语句各自的执行次数,以及其中所含基本操作的数目

#### 最坏、最好与平均情况

![image-20210704233544642](%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA.assets/image-20210704233544642.png)

以大$\sigma$记号形式表示的时间复杂度,实质上是对算法执行时间的一种保守估计-对于规模为n的任意输入,算法的运行时间都不会超过$\sigma(f(n))$.的确需要这么长计算时间的输入实例,称作最坏实例或最坏情况(worst case)

需强调的是,这种保守估计并不排斥更好情况甚至最好情况(best case)的存在和出现.当然,有时也需要考查所谓的平均情况(average case) ,也就是按照某种约定的概率分布,将规模为n的所有输入对应的计算时间加权平均.比较而言, “最坏情况复杂度”是人们最为关注且使用最多的,在一些特殊的场合甚至成为唯一的指标

#### 大$\Omega$记号

为了对算法的复杂度最好情况做出估计,需要借助另一个记号.如果存在正的常数c和函数(n),使得对于任何n >> 2都有
$$
T(n) \geq c\cdot g(n)
$$
就可以认为,在n足够大之后, g(n)给出了T(n)的一个渐进下界.此时,我们记之为:
$$
T(n) =\Omega(g(n))
$$
这里的$\Omega$称作“大$\Omega$记号” (big-omega notation) .与大$\sigma$号恰好相反,大$\Omega$记号是对算法执行效率的乐观估计-对于规模为n的任意输入,算法的运行时间都不低于$\Omega(g(n))$

#### 大$\Theta$记号

借助大$\sigma$记号、大$\Omega$记号,可以对算法的时间复杂度作出定量的界定,亦即,从渐进的趋势看, T(n)介于$\Omega(g(n))$与$\sigma(f(n))$之间.若恰巧出现g(n) =f(n)的情况,则可以使用另一记号来表示

如果存在正的常数$c1 < c2$和函数h(n),使得对于任何n >> 2都有
$$
c_1\cdot h(n) \leq T(n) \leq c_2\cdot h(n)
$$
就可以认为在n足够大之后, h(n)给出了T(n)的一个确界.此时,我们记之为:
$$
T(n) =\Theta(h(n))
$$
这里的$\Theta$称作“大$\Theta$记号” (big-theta notation) ,它是对算法复杂度的准确估计.对于规模为n的任何输入,算法的运行时间T(n)都与$\Theta(h(n))$同阶

### 空间复杂度

除了执行时间的长短,算法所需存储空间的多少也是衡量其性能的一个重要方面,此即所谓的空间复杂度(space complexity) .实际上,以上针对时间复杂度所引入的几种渐进记号,也适用于对空间复杂度的度量,其原理及方法基本相同,不再赘述

需要注意的是,为了更为客观地评价算法性能的优劣,除非特别申明,空间复杂度通常并不计入原始输入本身所占用的空间-对于同一问题,这一指标对任何算法都是相同的.反之,其它(如转储、中转、索引、映射、缓冲等)各个方面所消耗的空间,则都应计入.

另外,很多时候我们都是更多地甚至仅仅关注于算法的时间复杂度,而不必对空间复杂度做专门的考查.这种简便评测方式的依据,来自于以下事实:==就渐进复杂度的意义而言,在任一算法的任何一次运行过程中所消耗的存储空间,都不会多于其间所执行基本操作的累计次数==

实际上根据定义,每次基本操作所涉及的存储空间,都不会超过常数规模;纵然每次基本操作所占用或访问的存储空间都是新开辟的,整个算法所需的空间总量,也不过与基本操作的次数同阶.==从这个意义上说,时间复杂度本身就是空间复杂度的一个天然的上界==

当然,对空间复杂度的分析也有其自身的意义,尤其在对空间效率非常在乎的应用场合中,或当问题的输入规模极为庞大时,由时间复杂度所确立的平凡上界已经难以令人满意.这类情况下,人们将更为精细地考查不同算法的空间效率,并尽力在此方面不断优化

## 复杂度分析

### 复杂度层次

![image-20210709201741465](%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA.assets/image-20210709201741465.png)

##### $\sigma(n)$

运行时间可表示和度量为$T(n) =\sigma(1)$的这一类算法,统称作“常数时间复杂度算法”(constant-time algorithm) .==此类算法已是最为理想的==,因为不可能奢望“不劳而获”

一般地,仅含一次或常数次基本操作的算法均属此类.此类算法.通常不含循环、分支、子程序调用等,但也不能仅凭语法结构的表面形式一概而论

#### 对数$\sigma(\log n)$

由大$\sigma$记号定义,在用函数$\log_rn$界定渐进复杂度时,常底数r的具体取值无所谓,故通常不予专门标出而笼统地记作$\log n$.此类算法称作具有“对数时间复杂度” (logarithmic-time algorithm)

#### 对数多项式复杂度

更一般地,凡运行时间可以表示和度量为$T(n) =\sigma(\log ^cn)$形式的这一类算法(其中常数c>0),均统称作“对数多项式时间复杂度的算法” (polylogarithmic-time algorithm).上述$\sigma(\log n )$即c=1的特例.此类算法的效率虽不如常数复杂度算法理想,但从多项式的角度看仍能无限接近于后者,故也是极为高效的一类算法

#### 线性$\sigma(n)$

凡运行时间可以表示和度量为$T(n) = \sigma(n)$形式的这一类算法,均统称作“线性时间复杂度算法” (linear-time algorithm) .也就是说,对于输入的每一单元,此类算法平均消耗常数时间.就大多数问题而言,在对输入的每一单元均至少访问一次之前,不可能得出解答

#### 多项式$\sigma(polynomial(n))$

若运行时间可以表示和度量为$T(n) =\sigma(f(n))$的形式,而且$f(x)$为多项式,则对应的算法称作“多项式时间复杂度算法” (polynomial-time algorithm) 

在算法复杂度理论中,多项式时间复杂度被视作一个具有特殊意义的复杂度级别.多项式级的运行时间成本,在实际应用中一般被认为是可接受的或可忍受的.某问题若存在一个复杂度在此范围以内的算法,则称该问题是可有效求解的或易解的(tractable)

#### 指数$\sigma(2^n)$

一般地,凡运行时间可以表示和度量为$T(n) = \sigma(a^n)$形式的算法(a > 1) ,均属于“指数时间复杂度算法” (exponential-time algorithm) 

#### 从多项式到指数

从常数、对数、线性、平方到多项式时间复杂度,算法效率的差异还在可接受的范围.然而,在多项式与指数时间复杂度之间,却有着一道巨大的鸿沟.当问题规模较大后,指数复杂度算法的实际效率将急剧下降,计算时间之长很快就会达到令人难以忍受的地步.因此通常认为,指数复杂度算法无法真正应用于实际问题中,它们不是有效算法,甚至不能称作算法.相应地,不存在多项式复杂度算法的问题,也称作难解的(intractable)问题

### 输入规模

对算法复杂度的界定,都是相对于问题的输入规模而言的.然而,细心的读者可能已经注意到,不同的人在不同场合下关于“输入规模”的理解、定义和度量可能不尽相同,因此也可能导致复杂度分析的结论有所差异

严格地说,所谓待计算问题的输入规模,应严格定义为“用以描述输入所需的空间规模”

## 递归

==分支转向是算法的灵魂;函数和过程及其之间的相互调用,是在经过抽象和封装之后,实现分支转向的一种重要机制;而递归则是函数和过程调用的一种特殊形式,即允许函数和过程进行自我调用==.因其高度的抽象性和简洁性,递归已成为多数高级程序语言普遍支持的一项重要特性.比如在C++语言中,递归调用(recursive call)就是某一方法调用自身.这种自我调用通常是直接的,即在函数体中包含一条或多条调用自身的语句.递归也可能以间接的形式出现,即某个方法首先调用其它方法,再辗转通过其它方法的相互调用,最终调用起始的方法自身

递归的价值在于,许多应用问题都可简洁而准确地描述为递归形式.以操作系统为例,多数文件系统的目录结构都是递归定义的.具体地,每个文件系统都有一个最顶层的目录,其中可以包含若干文件和下一层的子目录;而在每一子目录中,也同样可能包含若干文件和再下一层的子目录;如此递推,直至不含任何下层的子目录.通过如此的递归定义,文件系统中的目录就可以根据实际应用的需要嵌套任意多层(只要系统的存储资源足以支持).递归也是一种基本而典型的算法设计模式.这一模式可以对实际问题中反复出现的结构和形式做高度概括,并从本质层面加以描述与刻画,进而导出高效的算法.==从程序结构的角度看,递归模式能够统筹纷繁多变的具体情况,避免复杂的分支以及嵌套的循环,从而更为简明地描述和实现算法,减少代码量,提高算法的可读性,保证算法的整体效率==

### 递推方程

递归算法的另一常用分析方法,即递推方程(recurrence equation)法.与递归跟踪分析相反,该方法无需绘出具体的调用过程,而是通过对递归模式的数学归纳,导出复杂度定界函数的递推方程(组)及其边界条件,从而将复杂度的分析,转化为递归方程(组)的求解

在总体思路上,该方法与微分方程法颇为相似:很多复杂函数的显式表示通常不易直接获得,但是它们的微分形式却往往遵循某些相对简洁的规律,通过求解描述这些规律的一组微分方程,即可最终导出原函数的显式表示.微分方程的解通常并不唯一,除非给定足够多的边界条件.类似地,为使复杂度定界函数的递推方程能够给出确定的解,也需要给定某些边界条件.以下我们将看到,这类边界条件往往可以通过对递归基的分析而获得

### 递归模式

#### 多递归基

为保证有穷性,递归算法都必须设置递归基,且确保总能执行到.为此,针对每一类可能出现的平凡情况,都需设置对应的递归基,故同一算法的递归基可能(显式或隐式地)不止一个

#### 实现递归

在设计递归算法时,往往需要从多个角度反复尝试,方能确定对问题的输入及其规模的最佳,划分方式.有时,还可能需要从不同的角度重新定义和描述原问题,使得经分解所得的子问题与原问题具有相同的语义形式

#### 多向递归

递归算法中,不仅递归基可能有多个,递归调用也可能有多种可供选择的分支

### 递归消除

#### 空间成本

从递归跟踪分析的角度不难看出,递归算法所消耗的空间量主要取决于递归深度,故较之同一算法的迭代版,递归版往往需耗费更多空间,并进而影响实际的运行速度

另外,就操作系统而言,为实现递归调用需要花费大量额外的时间以创建、维护和销毁各递归实例,这些也会令计算的负担雪上加霜.有鉴于此,在对运行速度要求极高、存储空间需精打细算的场合,往往应将递归算法改写成等价的非递归版本.一般的转换思路,无非是利用栈结构模拟操作系统的工作过程

#### 尾递归及其消除

在线性递归算法中,若递归调用在递归实例中恰好以最后一步操作的形式出现,则称作尾递归(tail recursion) 

尾递归的判断应依据对算法实际执行过程的分析,而不仅仅是算法外在的语法形式.比如,递归语句出现在代码体的最后一行,并不见得就是尾递归;严格地说,只有当该算法(除平凡递归基外)任一实例都终止于这一递归调用时,才属于尾递归

### 二分递归

#### 分而治之

将问题分解为若干规模更小的子问题,再通过递归机制分别求解.这种分解持续进行,直到子问题规模缩减至平凡情况.这也就是所谓的分而治之(divide-and-conquer)策略

#### 优化策略

为消除递归算法中重复的递归实例,一种自然而然的思路和技巧,可以概括为:==借助一定量的辅助空间,在各子问题求解之后,及时记录下其对应的解答==

比如,可以从原问题出发自顶而下,每当遇到一个子问题,都首先查验它是否已经计算过,以期通过直接调阅记录获得解答,从而避免重新计算.也可以从递归基出发,自底而上递推地得出各子问题的解,直至最终原问题的解.前者即所谓的制表(tabulation)或记忆(memoization)策略,后者即所谓的动态规划(dynamic programming)策略

